{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32717f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = \"ls__YOUR_KEY\"   # REQUIRED\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"ls__YOUR_KEY\"   # optional fallback\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"multi-agent-pipeline-demo\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98148ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_ollama import ChatOllama\n",
    "from typing import TypedDict\n",
    "\n",
    "# --------------- LANGSMITH ENABLED ----------------\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"multi-agent-pipeline-demo\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"<your-key>\"\n",
    "\n",
    "# --------------- MODEL ----------------------------\n",
    "llm = ChatOllama(model=\"mistral\")\n",
    "\n",
    "# --------------- STATE ----------------------------\n",
    "class PipelineState(TypedDict):\n",
    "    task: str\n",
    "    plan: str\n",
    "    research: str\n",
    "    code: str\n",
    "    test_results: str\n",
    "\n",
    "# --------------- AGENTS ----------------------------\n",
    "def planner(state: PipelineState):\n",
    "    out = llm.invoke(f\"Break this task into 3 steps:\\n{state['task']}\")\n",
    "    return {\"plan\": out.content}\n",
    "\n",
    "def researcher(state: PipelineState):\n",
    "    out = llm.invoke(f\"Provide research for this plan:\\n{state['plan']}\")\n",
    "    return {\"research\": out.content}\n",
    "\n",
    "def coder(state: PipelineState):\n",
    "    out = llm.invoke(f\"Write Python code using:\\n{state['research']}\")\n",
    "    return {\"code\": out.content}\n",
    "\n",
    "def tester(state: PipelineState):\n",
    "    out = llm.invoke(f\"Test this code:\\n{state['code']}\")\n",
    "    return {\"test_results\": out.content}\n",
    "\n",
    "# --------------- GRAPH ----------------------------\n",
    "g = StateGraph(PipelineState)\n",
    "\n",
    "g.add_node(\"planner\", planner)\n",
    "g.add_node(\"researcher\", researcher)\n",
    "g.add_node(\"coder\", coder)\n",
    "g.add_node(\"tester\", tester)\n",
    "\n",
    "g.set_entry_point(\"planner\")\n",
    "\n",
    "g.add_edge(\"planner\", \"researcher\")\n",
    "g.add_edge(\"researcher\", \"coder\")\n",
    "g.add_edge(\"coder\", \"tester\")\n",
    "g.add_edge(\"tester\", END)\n",
    "\n",
    "app = g.compile()\n",
    "\n",
    "# --------------- RUN ------------------------------\n",
    "result = app.invoke({\"task\": \"Build a basic calculator in Python\"})\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
